{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd66272b",
   "metadata": {},
   "source": [
    "# Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdde701",
   "metadata": {},
   "source": [
    "## Visão geral do Apache Spark\n",
    "\n",
    "    O Spark é um mecanismo de análise de processamento paralelo unificado para processamento de dados em larga escla. Ele oferece suporte ao processamento na memória para aumentar o desempenho de aplicativos que analisam big data. Forenece APIs de alto nível para Java, Scala, Python e um mecanismo otimizado que oferece suporte a gráficos de execução geral. Ele também oferece um conjunto de ferramentas auxiliares incluindo processamento de gráficos(GraphX), Processametno de dados estruturados (Spark SQL), compatibilidade com Pandas, para aprendizado de máquina como MLib e streaming estruturado para computação. Ela foi originalmente desenvolvida na Universidade de Berkeley em 2009. O framework Spark é 100% open source, hospedado no Apache Software Foundation independente de fornecedor. Ele usa o modelo de programação `MapReduce` usado pelo Hadoop e o modelo de programação estendido. Fornecendo uma perfomance muito superior ao Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64764d4",
   "metadata": {},
   "source": [
    "### Arquitetura do Spark\n",
    "\n",
    "    Para executar o conjunto de bibliotecas e ferramentas que acompanham o Spark ele útiliza três partes principais:\n",
    "    1. **Drive Program**: É quem faz a tradução, configuração e gestão dos comando passados para a API.\n",
    "    2. **Cluster Manager**: é um componentte opcional que só é necessário se o Spark for executado de forma distribuída.\n",
    "    3. **Workers**: São as máquinas que executarão as tarefas que são enviadas pelo Drive Program, caso o Spark seja executado localmente, a máquina desempenhará os papéis de Driver Program e Worker.\n",
    "    \n",
    " ![Arquitetura Spark](https://sparkbyexamples.com/wp-content/uploads/2020/02/spark-cluster-overview.png) \n",
    " \n",
    "### Tipos de gerenciamento de Cluester\n",
    " \n",
    " - **Autônomo**: um gerenciador de cluster simples incluído no Spark que facilita a configuração de um Cluster.\n",
    " - **Mesos**: gerenciador de cluster que pode executar aplicativos Hadoop MapReduce e PySpark.\n",
    " - **Hadoop YARN**: gerenciador de recursos no Hadoop 2. É o mais usado.\n",
    " - **Kubernetes**: gerenciador de aplicativos em contêineres."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75de0472",
   "metadata": {},
   "source": [
    "### Principais vantagens\n",
    "\n",
    "- **Integração**: Ela possui recursos complexos, revelantes e a possibilidade de integração com ferramentas conhecidas com Apache Cassandra e Hadoop.\n",
    "- **Quantidade de clusters**: Muitas organizações executam o Spark em clusters de milhares de nós. O maior aglomerado que conhecemos tem 8.000 deles. Em termos de tamanho de dados, o Spark demonstrou funcionar bem até petabytes.\n",
    "- **API para linguagens de programação**: Suporta linguagens como Java, Python, R e Scal e SQL. \n",
    "- **Eficiência**: Ele foi planejato para tirar um consumo eficiente de recursos da máquina, ele é capaz de processar os dados de forma a minimizar a quantidade de dados que precisam ser lidos.\n",
    "- **Ampla biblioteca de ferramentas**: Similar a outras bibliotecas para análise e processamento de dados, o Spark possui algumas bibliotecas para facilitar do desenvolvedores. processamento de gráficos(GraphX), Processametno de dados estruturados (Spark SQL), compatibilidade com Pandas, para aprendizado de máquina como MLib, streaming estruturado para computação(Spark Streaming).\n",
    "- **Framework de código aberto**: O biblioteca inicialmente comeu a ser desenvolvida no AMPLab e posteriormente é mantido pela Apache Software.\n",
    "- **Velocidade**: Ela é capaz de processar grandes quantidades de dados em paralelo e processamento na memória."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968a76cf",
   "metadata": {},
   "source": [
    "### Algumas Desvantagens\n",
    "\n",
    "- **Overhead de memória**: Como ele faz processamento em memória, a quantidade de memória ram que seus jobs utilizam pode ser um problema gerenciar o consumo no cluster, mas caso o volume de dados ultrapassar a quantidade de memória a plataforma começa a usar o armazenamento em disco por padrão, causando mais **Latência de E/S**.\n",
    "- **Latência de E/S**: o Apache Spark pode sofrer com latência de E/S (entrada/saída) em alguns casos, especialmente quando os dados precisam ser lidos de armazenamento secundário."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f100840",
   "metadata": {},
   "source": [
    "## Ecossistema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f450419c",
   "metadata": {},
   "source": [
    "O Apache Spark se integra com diversas bibliotecas que podem te auxiliar na integração com outros serviços.\n",
    "\n",
    "- **Data science e Machine learning**: Scikit learn, PyTorch, pandas, TTensorFlow, mlFlow, R.\n",
    "- **SQL analytics and BI**: Apache Superset, Power BI, Loocker, Re dash, Tableau, dbt, \n",
    "- **Infra e armazenamento**: Elasticsearch, MongoDB, Apache kafka, Delta Lake, Kubernetes, Apache Airflow, Parquet, SQL Server, cassandra, Apache ORC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df556fa",
   "metadata": {},
   "source": [
    "## Instalação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c767d9b",
   "metadata": {},
   "source": [
    "### Prerequisitos\n",
    "\n",
    "1. Hadoop 3.3.0\n",
    "2. OpenJDK e JRE 8 ou superior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b10748",
   "metadata": {},
   "source": [
    "### Instalação manual dos pacotes...\n",
    "\n",
    "- Instalando o Scala\n",
    "\n",
    "```\n",
    "\n",
    "wget https://downloads.lightbend.com/scala/2.13.3/scala-2.13.3.tgz\n",
    "\n",
    "tar xvf scala-2.13.3.tgz\n",
    "\n",
    "```\n",
    "\n",
    ">adicionar Scala e um alias para o jupyter notebook no bashrc\n",
    ">```\n",
    ">nano ~/.bashrc\n",
    ">```\n",
    ">adicione esse comandos no final do arquivo\n",
    ">alias jupyter-notebook=”~/.local/bin/jupyter-notebook — no-browser”\n",
    ">export SCALA_HOME=/home/`nome do root user`/scala-2.13.3\n",
    ">export PATH=$PATH:$SCALA_HOME/bin\n",
    ">ctrl + o para salvar e ctrl + x para fechar\n",
    ">execute o arquivo\n",
    ">```\n",
    ">source ~/.bashrc\n",
    ">```\n",
    "\n",
    "- Instalando Spark com Hadoop\n",
    "\n",
    "```\n",
    "wget https://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
    "```\n",
    "```\n",
    "tar xvf spark-3.1.1-bin-hadoop3.2.tgz\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f488972e",
   "metadata": {},
   "source": [
    "### Iniciando projeto com virtualenv e jupyter notebook\n",
    "\n",
    "1. Instalando JDK e JRE\n",
    "\n",
    "```\n",
    "\n",
    "!sudo apt update\n",
    "!sudo apt install default-jre\n",
    "!sudo apt install default-jdk\n",
    "!java -version\n",
    "!javac -version\n",
    "\n",
    "```\n",
    ">É necessário ter Java 8 ou posterior instalado como JAVA_HOME \n",
    ">Caso estéja usando o JDK 11, \n",
    ">defina '-dio.netty.tryReflectionSetAccessible=true' para recursos relacionados ao Arrow\n",
    "\n",
    "2. checar atualizações do python3 e pip\n",
    "\n",
    "```\n",
    "\n",
    "!sudo apt install python3 python3-pip ipython3\n",
    "\n",
    "```\n",
    "\n",
    "3. Crie uma virtal env\n",
    "\n",
    "```\n",
    "!python3 -m venv <nome_do_diretório>\n",
    "```\n",
    "\n",
    "4. Entre no diretório e execute\n",
    "\n",
    "```\n",
    "!cd <nome_do_diretório>\n",
    "!source bin/activate\n",
    "```\n",
    "\n",
    "5. Intale jupyter py4j, pyspark e jupyter-notebook\n",
    "\n",
    "```\n",
    "!pip install notebook py4j pyspark \n",
    "```\n",
    ">caso necessite de alguma biblioteca extra do pyspark use a variação\n",
    ">\n",
    ">```\n",
    ">!pip install pyspark[nome_do_pacote] \n",
    ">```\n",
    ">\n",
    ">para plotar seus dados, você pode usar a lib plotly...\n",
    ">\n",
    ">```\n",
    ">pip install pyspark[pandas_on_spark] plotly  \n",
    ">```\n",
    "\n",
    ">Caso necessite de outras versões do Hadoop\n",
    ">Por padrão são usadas as libs Hadoop 3.3 e Hive 2.3\n",
    ">```\n",
    ">!PYSPARK_HADOOP_VERSION=2 pip install pyspark\n",
    ">```\n",
    ">```\n",
    ">PYSPARK_HIVE_VERSION=2 pip install pyspark -v\n",
    ">```\n",
    ">tag -v para exibir o status do download\n",
    ">Valores suportados em 'PYSPARK_HADOOP_VERSION'\n",
    ">2: Apache Hadoop 2.7\n",
    ">3: Apache Hadoop 3.3 (default)\n",
    "\n",
    "6. Execute o notebook\n",
    "\n",
    "```\n",
    "!jupyter notebook \n",
    "```\n",
    ">por padrão a porta de acesso para o recurso é 8888\n",
    ">caso esteje usando wsl, copie e cole o link com o token de acesso no navegador de sua preferência\n",
    ">acesse http://localhost:8888/ no navegador..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b639a6",
   "metadata": {},
   "source": [
    " ## Dependências PySpark\n",
    " \n",
    " |Dependências usadas | Versão mínima suportada | Observações|\n",
    " |---|---|---|\n",
    " | `pandas` | 1.0.5 | Opcional para o **Spark SQL** e obrigatória para o **pandas API Spark** |\n",
    " | `pyarrow` | 1.0.0 | Opcional para o **spark SQL** e obrigatória para o **pandas API Spark** |\n",
    " | `numpy` | 1.15 | Obrigatória para **API pandas** e **DataFrame-basedAPI** do **MLLib** |\n",
    " | `py4j` | 0.10.9.5 | Obrigatória |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d6dbbe",
   "metadata": {},
   "source": [
    "## Iniciando sessão Pyspark\n",
    "\n",
    "Para ter acesso aos recursos do PySpark usamos a classe `SparkSession` para iniciar o cluter e a sessão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ecbc98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!export SPARK_HOME='/MLOps/ETL_PySpark/ETL/lib/python3.8/site-packages/pyspark'\n",
    "!export PYSPARK_PYTHON='/MLOps/ETL_PySpark/ETL/bin/python3.8'\n",
    "#!export PYSPARK_DRIVER_PYTHON='/MLOps/ETL_PySpark/ETL/bin/jupyter'\n",
    "#!export PYSPARK_DRIVER_PYTHON_OPTS='notebook'\n",
    "#!export SPARK_LOCAL_IP=192.168.1.110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b66b1196",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/16 16:04:48 WARN Utils: Your hostname, DESKTOP-LI0TJ46 resolves to a loopback address: 127.0.1.1; using 192.168.1.110 instead (on interface eth0)\n",
      "23/01/16 16:04:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/16 16:04:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13092cf1",
   "metadata": {},
   "source": [
    ">Após iniciarmos a sessão, é possível monitorar os recursos do spark.\n",
    ">Executando o Spark localmente a url de acesso a esse recurso é `localhost:4040`.\n",
    ">Caso necessário é possível mudar o ip e porta de acesso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4278e192",
   "metadata": {},
   "source": [
    "\n",
    "## Resumo rápido sobre DataFrame\n",
    "\n",
    "Para usar aplicações PySpark precisamos inicializar o ponto de entrada da lib usando o `SparkSession`\n",
    "\n",
    ">Caso deseje execute via shell por meio do executável do pyspark, o shel cria automaticamente a sessão na variável spark para os usuários."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53276514",
   "metadata": {},
   "source": [
    "### Criando DataFrame\n",
    "\n",
    "Use a função `pyspark.sql.SparkSession.createDataFrame` passando uma lista de listas, tuplas, difionários e `pyspark.sql.Row`, um `Pandas DataFrame` e um RDD que consiste em tal lista. essa função usa o `schema` para especificar o esquema do DataFrame. Quando omisso, o PySpark se encarrega de criar um esquema correspondente obtendo uma amostra dos dados.\n",
    "\n",
    "Exemplo de criação de DataFrame usando a partir de uma lista de linhas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "072e51b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  4|5.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
    "])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b61ee1",
   "metadata": {},
   "source": [
    "### Criando DataFrame passando um `schema`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c17a9e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
    "], schema='a long, b double, c string, d date, e timestamp')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c53118",
   "metadata": {},
   "source": [
    "### Usando o pyspark.pandas para criar um DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac2955d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "/home/luizpibo/MLOps/ETL_PySpark/ETL/lib/python3.8/site-packages/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  fields = [\n",
      "/home/luizpibo/MLOps/ETL_PySpark/ETL/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4</td>\n",
       "      <td>400</td>\n",
       "      <td>four</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>five</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>6</td>\n",
       "      <td>600</td>\n",
       "      <td>six</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    a    b      c\n",
       "10  1  100    one\n",
       "20  2  200    two\n",
       "30  3  300  three\n",
       "40  4  400   four\n",
       "50  5  500   five\n",
       "60  6  600    six"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "\n",
    "psdf = ps.DataFrame(\n",
    "    {'a': [1, 2, 3, 4, 5, 6],\n",
    "     'b': [100, 200, 300, 400, 500, 600],\n",
    "     'c': [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\"]},\n",
    "    index=[10, 20, 30, 40, 50, 60])\n",
    "psdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772a2a34",
   "metadata": {},
   "source": [
    "## Referências\n",
    "\n",
    "- [APACHE. Apache Spark, 2022. Visão geral do Spark](https://spark.apache.org/docs/latest/)\n",
    "- [Microsoft, O que é o Apache Spark?](https://learn.microsoft.com/pt-br/dotnet/spark/what-is-spark)\n",
    "- [Cetax, Spark: Saiba mais sobre esse poderoso framework](https://cetax.com.br/conheca-mais-sobre-o-framework-apache-spark/)\n",
    "- [Spark: entenda sua função e saiba mais sobre essa ferramenta.](https://blog.xpeducacao.com.br/apache-spark/)\n",
    "- [Guia Spark SQL, DataFrames e DataSets](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "- [Guia RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
    "- [Guia Programação de Streaming estruturado](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)\n",
    "- [Guia Programação em Streaming](https://spark.apache.org/docs/latest/streaming-programming-guide.html)\n",
    "- [Guia da lib de ML (MLlib)](https://spark.apache.org/docs/latest/ml-guide.html)\n",
    "- [Entendendo o funcionamento do PySpark](https://medium.com/data-hackers/entendo-funcionamento-do-pyspark-2b5ab4321ab9)\n",
    "- [Spark with Python (PySpark) Tutorial For Beginners](https://sparkbyexamples.com/pyspark-tutorial/)\n",
    "- [Perguntas frequentes Do Apache Spark ™](https://spark.apache.org/faq.html)\n",
    "- [Uma breve introdução do Hadoop HDFS — Hadoop Distributed File System [1|2]](https://medium.com/@cm.oeiras01/uma-breve-introdu%C3%A7%C3%A3o-do-hadoop-hdfs-hadoop-distributed-file-system-1-2-6883710ea64f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
